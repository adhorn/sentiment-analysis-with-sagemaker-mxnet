{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with MXNet on SageMaker\n",
    "\n",
    "In this notebook, we will build and train a sentiment analysis model with MXNet on SageMaker.\n",
    "Our model will learn to classify movie reviews as positive (1) or negative (0).\n",
    "\n",
    "We will use the SST-2 dataset (Stanford Sentiment Treebank 2), which consists of of movie reviews with one sentence per review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Initialization and imports\n",
    "\n",
    "We will start by importing the modules needed, and creating a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset download and preparation\n",
    "\n",
    "Next let's download the datasets into a /data dir, and then upload it to SageMaker's S3 bucket.\n",
    "Each line in the dataset has space separated tokens, the first token being the label: 1 for positive and 0 for negative.\n",
    "\n",
    "We can also check out the downloaded files in Jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training data. We're downloading the Stanford Sentiment dataset\n",
    "# https://nlp.stanford.edu/sentiment/index.html\n",
    "\n",
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/stsa.binary.phrases.train > data/train\n",
    "!curl https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/stsa.binary.test > data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-968277166688\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/sentiment-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the training function\n",
    "\n",
    "Now we will wanto to implement the training logic that will run on the SageMaker platform. \n",
    "The training scripts are essentially the same as one you would write for local training,  except that you need to provide a train function with a specific signature. \n",
    "\n",
    "When SageMaker calls your function, it will pass in arguments that describe the training environment. Let's checkout the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd, nd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "import re\r\n",
      "from mxnet.io import DataIter, DataBatch, DataDesc\r\n",
      "import bisect, random\r\n",
      "from collections import Counter\r\n",
      "from itertools import chain, islice\r\n",
      "\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 8)\r\n",
      "    epochs = hyperparameters.get('epochs', 2)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.01)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 1000)\r\n",
      "    embedding_size = hyperparameters.get('embedding_size', 50)\r\n",
      "\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    ctx = mx.gpu() if num_gpus > 0 else mx.cpu()\r\n",
      "\r\n",
      "    training_dir = channel_input_dirs['training']\r\n",
      "    train_sentences, train_labels, _ = get_dataset(training_dir + '/train')\r\n",
      "    val_sentences, val_labels, _ = get_dataset(training_dir + '/test')\r\n",
      "\r\n",
      "    num_classes = len(set(train_labels))\r\n",
      "    vocab = create_vocab(train_sentences)\r\n",
      "    vocab_size = len(vocab)\r\n",
      "\r\n",
      "    train_sentences = [[vocab.get(token, 1) for token in line if len(line)>0] for line in train_sentences]\r\n",
      "    val_sentences = [[vocab.get(token, 1) for token in line if len(line)>0] for line in val_sentences]\r\n",
      "\r\n",
      "    # Alternatively to splitting in memory, the data could be pre-split in S3 and use ShardedByS3Key\r\n",
      "    # to do parallel training.\r\n",
      "    shard_size = len(train_sentences) // len(hosts)\r\n",
      "    for i, host in enumerate(hosts):\r\n",
      "        if host == current_host:\r\n",
      "            start = shard_size * i\r\n",
      "            end = start + shard_size\r\n",
      "            break\r\n",
      "\r\n",
      "    train_iterator = BucketSentenceIter(train_sentences[start:end], train_labels[start:end], batch_size)\r\n",
      "    val_iterator = BucketSentenceIter(val_sentences, val_labels, batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = TextClassifier(vocab_size, embedding_size, num_classes)\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'adam',\r\n",
      "                            {'learning_rate': learning_rate},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        i = 0\r\n",
      "        for batch in train_iterator:\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = batch.data[0].as_in_context(ctx)\r\n",
      "            label = batch.label[0].as_in_context(ctx)\r\n",
      "\r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "                L.backward()\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "            # update metric at last.\r\n",
      "            metric.update([label], [output])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "            i += 1\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_iterator)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "        train_iterator.reset()\r\n",
      "    return net, vocab\r\n",
      "\r\n",
      "\r\n",
      "class BucketSentenceIter(DataIter):\r\n",
      "    \"\"\"Simple bucketing iterator for text classification model.\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    sentences : list of list of int\r\n",
      "        Encoded sentences.\r\n",
      "    labels : list of int\r\n",
      "        Corresponding labels.\r\n",
      "    batch_size : int\r\n",
      "        Batch size of the data.\r\n",
      "    buckets : list of int, optional\r\n",
      "        Size of the data buckets. Automatically generated if None.\r\n",
      "    invalid_label : int, optional\r\n",
      "        Key for invalid label, e.g. <unk. The default is 0.\r\n",
      "    dtype : str, optional\r\n",
      "        Data type of the encoding. The default data type is 'float32'.\r\n",
      "    data_name : str, optional\r\n",
      "        Name of the data. The default name is 'data'.\r\n",
      "    label_name : str, optional\r\n",
      "        Name of the label. The default name is 'softmax_label'.\r\n",
      "    layout : str, optional\r\n",
      "        Format of data and label. 'NT' means (batch_size, length)\r\n",
      "        and 'TN' means (length, batch_size).\r\n",
      "    \"\"\"\r\n",
      "    def __init__(self, sentences, labels, batch_size, buckets=None, invalid_label=0,\r\n",
      "                 data_name='data', label_name='softmax_label', dtype='float32',\r\n",
      "                 layout='NT'):\r\n",
      "        super(BucketSentenceIter, self).__init__()\r\n",
      "        if not buckets:\r\n",
      "            buckets = [i for i, j in enumerate(np.bincount([len(s) for s in sentences]))\r\n",
      "                       if j >= batch_size]\r\n",
      "        buckets.sort()\r\n",
      "\r\n",
      "        ndiscard = 0\r\n",
      "        self.data = [[] for _ in buckets]\r\n",
      "        self.labels = [[] for _ in buckets]\r\n",
      "        for i, sent in enumerate(sentences):\r\n",
      "            buck = bisect.bisect_left(buckets, len(sent))\r\n",
      "            if buck == len(buckets):\r\n",
      "                ndiscard += 1\r\n",
      "                continue\r\n",
      "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\r\n",
      "            buff[:len(sent)] = sent\r\n",
      "            self.data[buck].append(buff)\r\n",
      "            self.labels[buck].append(labels[i])\r\n",
      "\r\n",
      "        self.data = [np.asarray(i, dtype=dtype) for i in self.data]\r\n",
      "        self.labels = [np.asarray(i, dtype=dtype) for i in self.labels]\r\n",
      "\r\n",
      "        print(\"WARNING: discarded %d sentences longer than the largest bucket.\"%ndiscard)\r\n",
      "\r\n",
      "        self.batch_size = batch_size\r\n",
      "        self.buckets = buckets\r\n",
      "        self.data_name = data_name\r\n",
      "        self.label_name = label_name\r\n",
      "        self.dtype = dtype\r\n",
      "        self.invalid_label = invalid_label\r\n",
      "        self.nddata = []\r\n",
      "        self.ndlabel = []\r\n",
      "        self.major_axis = layout.find('N')\r\n",
      "        self.layout = layout\r\n",
      "        self.default_bucket_key = max(buckets)\r\n",
      "\r\n",
      "        if self.major_axis == 0:\r\n",
      "            self.provide_data = [DataDesc(\r\n",
      "                name=self.data_name, shape=(batch_size, self.default_bucket_key),\r\n",
      "                layout=self.layout)]\r\n",
      "            self.provide_label = [DataDesc(\r\n",
      "                name=self.label_name, shape=(batch_size,),\r\n",
      "                layout=self.layout)]\r\n",
      "        elif self.major_axis == 1:\r\n",
      "            self.provide_data = [DataDesc(\r\n",
      "                name=self.data_name, shape=(self.default_bucket_key, batch_size),\r\n",
      "                layout=self.layout)]\r\n",
      "            self.provide_label = [DataDesc(\r\n",
      "                name=self.label_name, shape=(self.default_bucket_key, batch_size),\r\n",
      "                layout=self.layout)]\r\n",
      "        else:\r\n",
      "            raise ValueError(\"Invalid layout %s: Must by NT (batch major) or TN (time major)\")\r\n",
      "\r\n",
      "        self.idx = []\r\n",
      "        for i, buck in enumerate(self.data):\r\n",
      "            self.idx.extend([(i, j) for j in range(0, len(buck) - batch_size + 1, batch_size)])\r\n",
      "        self.curr_idx = 0\r\n",
      "        self.reset()\r\n",
      "\r\n",
      "    def reset(self):\r\n",
      "        \"\"\"Resets the iterator to the beginning of the data.\"\"\"\r\n",
      "        self.curr_idx = 0\r\n",
      "        random.shuffle(self.idx)\r\n",
      "        for i in range(len(self.data)):\r\n",
      "            data, labels =  self.data[i], self.labels[i]\r\n",
      "            p = np.random.permutation(len(data))\r\n",
      "            self.data[i], self.labels[i] = data[p], labels[p]\r\n",
      "\r\n",
      "        self.nddata = []\r\n",
      "        self.ndlabel = []\r\n",
      "        for buck,label_buck in zip(self.data, self.labels):\r\n",
      "            self.nddata.append(nd.array(buck, dtype=self.dtype))\r\n",
      "            self.ndlabel.append(nd.array(label_buck, dtype=self.dtype))\r\n",
      "\r\n",
      "    def next(self):\r\n",
      "        \"\"\"Returns the next batch of data.\"\"\"\r\n",
      "        if self.curr_idx == len(self.idx):\r\n",
      "            raise StopIteration\r\n",
      "        i, j = self.idx[self.curr_idx]\r\n",
      "        self.curr_idx += 1\r\n",
      "\r\n",
      "        if self.major_axis == 1:\r\n",
      "            data = self.nddata[i][j:j+self.batch_size].T\r\n",
      "            label = self.ndlabel[i][j:j+self.batch_size].T\r\n",
      "        else:\r\n",
      "            data = self.nddata[i][j:j+self.batch_size]\r\n",
      "            label = self.ndlabel[i][j:j+self.batch_size]\r\n",
      "\r\n",
      "        return DataBatch([data], [label], pad=0,\r\n",
      "                         bucket_key=self.buckets[i],\r\n",
      "                         provide_data=[DataDesc(\r\n",
      "                             name=self.data_name, shape=data.shape,\r\n",
      "                             layout=self.layout)],\r\n",
      "                         provide_label=[DataDesc(\r\n",
      "                             name=self.label_name, shape=label.shape,\r\n",
      "                             layout=self.layout)])\r\n",
      "\r\n",
      "\r\n",
      "class TextClassifier(gluon.HybridBlock):\r\n",
      "    def __init__(self, vocab_size, embedding_size, classes, **kwargs):\r\n",
      "        super(TextClassifier, self).__init__(**kwargs)\r\n",
      "        with self.name_scope():\r\n",
      "            self.dense = gluon.nn.Dense(classes)\r\n",
      "            self.embedding = gluon.nn.Embedding(input_dim=vocab_size, output_dim=embedding_size)\r\n",
      "\r\n",
      "    def hybrid_forward(self, F, x):\r\n",
      "        x = self.embedding(x)\r\n",
      "        x = F.mean(x, axis=1)\r\n",
      "        x = self.dense(x)\r\n",
      "        return x\r\n",
      "\r\n",
      "\r\n",
      "def get_dataset(filename):\r\n",
      "    labels = []\r\n",
      "    sentences = []\r\n",
      "    max_length = -1\r\n",
      "    with open(filename) as f:\r\n",
      "        for line in f:\r\n",
      "            tokens = line.split()\r\n",
      "            label = int(tokens[0])\r\n",
      "            words = tokens[1:]\r\n",
      "            max_length = max(max_length, len(words))\r\n",
      "            labels.append(label)\r\n",
      "            sentences.append(words)\r\n",
      "    return sentences, labels, max_length\r\n",
      "\r\n",
      "\r\n",
      "def create_vocab(sentences, min_count=5, num_words = 100000):\r\n",
      "    BOS_SYMBOL = \"<s>\"\r\n",
      "    EOS_SYMBOL = \"</s>\"\r\n",
      "    UNK_SYMBOL = \"<unk>\"\r\n",
      "    PAD_SYMBOL = \"<pad>\"\r\n",
      "    PAD_ID = 0\r\n",
      "    TOKEN_SEPARATOR = \" \"\r\n",
      "    VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\r\n",
      "    VOCAB_ENCODING = \"utf-8\"\r\n",
      "    vocab_symbols_set = set(VOCAB_SYMBOLS)\r\n",
      "    raw_vocab = Counter(token for line in sentences for token in line)\r\n",
      "    pruned_vocab = sorted(((c, w) for w, c in raw_vocab.items() if c >= min_count), reverse=True)\r\n",
      "    vocab = islice((w for c, w in pruned_vocab), num_words)\r\n",
      "    word_to_id = {word: idx for idx, word in enumerate(chain(VOCAB_SYMBOLS, vocab))}\r\n",
      "    return word_to_id\r\n",
      "\r\n",
      "\r\n",
      "def vocab_to_json(vocab, path):\r\n",
      "    with open(path, \"w\") as out:\r\n",
      "        json.dump(vocab, out, indent=4, ensure_ascii=True)\r\n",
      "        print('Vocabulary saved to \"%s\"', path)\r\n",
      "\r\n",
      "\r\n",
      "def vocab_from_json(path):\r\n",
      "    with open(path) as inp:\r\n",
      "        vocab = json.load(inp)\r\n",
      "        print('Vocabulary (%d words) loaded from \"%s\"', len(vocab), path)\r\n",
      "        return vocab\r\n",
      "\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    # save the model\r\n",
      "    net, vocab = net\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "    vocab_to_json(vocab, '%s/vocab.json' % model_dir)\r\n",
      "\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    val_data.reset()\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for batch in val_data:\r\n",
      "        data = batch.data[0].as_in_context(ctx)\r\n",
      "        label = batch.label[0].as_in_context(ctx)\r\n",
      "        output = net(data)\r\n",
      "        metric.update([label], [output])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    symbol = mx.sym.load('%s/model.json' % model_dir)\r\n",
      "    vocab = vocab_from_json('%s/vocab.json' % model_dir)\r\n",
      "    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\r\n",
      "    inputs = mx.sym.var('data')\r\n",
      "    param_dict = gluon.ParameterDict('model_')\r\n",
      "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "    return net, vocab\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    net, vocab = net\r\n",
      "    parsed = json.loads(data)\r\n",
      "    outputs = []\r\n",
      "    for row in parsed:\r\n",
      "        tokens = [vocab.get(token, 1) for token in row.split()]\r\n",
      "        nda = mx.nd.array([tokens])\r\n",
      "        output = net(nda)\r\n",
      "        prediction = mx.nd.argmax(output, axis=1)\r\n",
      "        outputs.append(int(prediction.asscalar()))\r\n",
      "    response_body = json.dumps(outputs)\r\n",
      "    return response_body, output_content_type"
     ]
    }
   ],
   "source": [
    "!cat 'sentiment-analysis.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training script on SageMaker\n",
    "\n",
    "SageMaker's MXNet class allows us to run our training function on SageMaker infrastructure. \n",
    "We need to configure it with our training script, an IAM role, the number of training instances, training instance type and hyper parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "m = MXNet(\"sentiment-analysis.py\", \n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.c5.4xlarge\",\n",
    "          hyperparameters={'batch_size': 8, \n",
    "                         'epochs': 2, \n",
    "                         'learning_rate': 0.01, \n",
    "                         'embedding_size': 50, \n",
    "                         'log_interval': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our MXNet object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-968277166688\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-06-15-07-17-04-550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................\n",
      "\u001b[31m2018-06-15 07:19:19,397 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-06-15 07:19:19,397 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-06-15 07:19:19,403 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2018-06-15 07:19:21,378 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'current_host': u'algo-1', u'network_interface_name': u'ethwe', u'hosts': [u'algo-1']}, 'user_script_name': u'sentiment-analysis.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'training': u'/opt/ml/input/data/training'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'sentiment-analysis.py', u'embedding_size': 50, u'learning_rate': 0.01, u'log_interval': 1000, u'epochs': 2, u'batch_size': 8, u'sagemaker_region': u'us-east-1', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-2018-06-15-07-17-04-550', u'sagemaker_container_log_level': 20, u'sagemaker_submit_directory': u's3://sagemaker-us-east-1-968277166688/sagemaker-mxnet-2018-06-15-07-17-04-550/source/sourcedir.tar.gz'}, 'hosts': [u'algo-1'], 'job_name': 'sagemaker-mxnet-2018-06-15-07-17-04-550', '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-east-1-968277166688/sagemaker-mxnet-2018-06-15-07-17-04-550/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-east-1', '_scheduler_ip': '10.32.0.4', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 16, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-968277166688/sagemaker-mxnet-2018-06-15-07-17-04-550/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-06-15 07:19:21,456 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-06-15 07:19:21,544 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-us-east-1-968277166688.s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-06-15 07:19:21,664 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31mWARNING: discarded 9 sentences longer than the largest bucket.\u001b[0m\n",
      "\u001b[31mWARNING: discarded 27 sentences longer than the largest bucket.\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 1000] Training: accuracy=0.732892, 2599.103950 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 2000] Training: accuracy=0.771739, 1920.798672 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 3000] Training: accuracy=0.796443, 1700.680791 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 4000] Training: accuracy=0.810547, 1450.876984 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 5000] Training: accuracy=0.820811, 1756.316776 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 6000] Training: accuracy=0.827425, 1575.399408 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 7000] Training: accuracy=0.833792, 1777.812440 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 8000] Training: accuracy=0.839114, 1616.146421 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 9000] Training: accuracy=0.843101, 1499.237389 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0] Training: accuracy=0.845577\u001b[0m\n",
      "\u001b[31m[Epoch 0] Validation: accuracy=0.803828\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 1000] Training: accuracy=0.904221, 1476.542662 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 2000] Training: accuracy=0.902924, 1667.383820 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 3000] Training: accuracy=0.901408, 1680.999549 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 4000] Training: accuracy=0.898744, 1728.896950 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 5000] Training: accuracy=0.897271, 1642.409790 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 6000] Training: accuracy=0.897788, 1624.675931 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 7000] Training: accuracy=0.897765, 1566.792678 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 8000] Training: accuracy=0.897028, 1559.728164 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 9000] Training: accuracy=0.897261, 1502.056135 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1] Training: accuracy=0.896309\u001b[0m\n",
      "\u001b[31m[Epoch 1] Validation: accuracy=0.794258\u001b[0m\n",
      "\u001b[31mVocabulary saved to \"%s\" /opt/ml/model/vocab.json\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 149\n"
     ]
    }
   ],
   "source": [
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting our trained model for inference\n",
    "\n",
    "As can be seen from the logs, we got > 80% accuracy on the test set.\n",
    "After training, we can host the trained MXNet model, and use it for inference.\n",
    "\n",
    "Let's deploy the model, starting with a single C5 instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-mxnet-2018-06-15-06-48-04-494\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-mxnet-2018-06-15-06-48-04-494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, instance_type='ml.c5.4xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the created predictor object and run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "data = [\"this was an awesome movie!\",\n",
    "        \"come on, you call this a movie?\",\n",
    "        \"best one I've seen in ages\",\n",
    "        \"i just could not watch it till the end.\",\n",
    "        \"the movie was so enthralling !\"]\n",
    "\n",
    "response = predictor.predict(data)\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, and do not need the endpoint any more, remember to delete the prediction endpoint to release the instance associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-mxnet-2018-06-15-06-48-04-494\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
